{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![image](https://github.com/IBM/watson-machine-learning-samples/raw/master/cloud/notebooks/headers/AutoAI-Banner_Incremental-learning-notebook.png)\n",
        "# Pipeline 5 Notebook for training continuation - AutoAI Notebook v1.16.9\n",
        "\n",
        "Consider these tips for working with an auto-generated notebook:\n",
        "- Notebook code generated using AutoAI will execute successfully. If you modify the notebook, we cannot guarantee it will run successfully.\n",
        "- This pipeline is optimized for the original data set. The pipeline might fail or produce sub-optimal results if used with different data.  If you want to use a different data set, consider retraining the AutoAI experiment to generate a new pipeline. For more information, see <a href=\"https://www.ibm.com/support/producthub/icpdata/docs/content/SSQNUZ_latest/wsj/analyze-data/autoai-notebook.html\">Cloud Pak for Data</a>. \n",
        "- Before modifying the pipeline or trying to re-fit the pipeline, consider that the code converts dataframes to numpy arrays before fitting the pipeline (a current restriction of the preprocessor pipeline).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"content\"></a>\n",
        "## Notebook content\n",
        "\n",
        "This notebook contains code to resume and continue training an AutoAI pipeline partially trained in an AutoAI experiment. If there is additional training data, the notebook retrieves the data in batches and incrementally trains the model, then tests the model.\n",
        "\n",
        "Some familiarity with Python is helpful. This notebook uses python 3.10 and scikit-learn 1.1.1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notebook goals\n",
        "\n",
        "This notebook introduces new commands and demonstrates techniques to support incremental learning, including: \n",
        "\n",
        "-  Data reader (read data in batches)\n",
        "-  Incremental learning (`partial_fit`)\n",
        "-  Pipeline evaluation\n",
        "\n",
        "## Contents\n",
        "\n",
        "This notebook contains the following parts:\n",
        "\n",
        "**[Setup](#setup)**<br>\n",
        "&nbsp;&nbsp;[Package installation](#install)<br>\n",
        "&nbsp;&nbsp;[AutoAI experiment metadata](#variables_definition)<br>\n",
        "&nbsp;&nbsp;[Watson Machine Learning connection](#connection)<br>\n",
        "**[Incremental learning](#incremental_learning)** <br>\n",
        "&nbsp;&nbsp;[Get pipeline](#preview_model_to_python_code)<br>\n",
        "&nbsp;&nbsp;[Read training data (DataLoader)](#data_loader)<br>\n",
        "&nbsp;&nbsp;[Incrementally train pipeline model](#train)<br>\n",
        "&nbsp;&nbsp;[Test pipeline model](#test_model)<br>\n",
        "**[Store the model](#saving)**<br>\n",
        "**[Create online deployment](#deployment)**<br>\n",
        "&nbsp;&nbsp;[Working with spaces](#working_spaces)<br>\n",
        "**[Summary and next steps](#summary_and_next_steps)**<br>\n",
        "**[Copyrights](#copyrights)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"setup\"></a>\n",
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"install\"></a>\n",
        "## Package installation\nBefore you use the sample code in this notebook, install the following packages:\n",
        " - ibm_watson_machine_learning,\n",
        " - autoai-libs,\n",
        " - scikit-learn,\n",
        " - snapml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install ibm-watson-machine-learning | tail -n 1\n",
        "!pip install autoai-libs==1.14.9 | tail -n 1\n",
        "!pip install scikit-learn==1.1.1 | tail -n 1\n",
        "!pip install 'snapml==1.8.12' | tail -n 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"variables_definition\"></a>\n",
        "## AutoAI experiment metadata\n",
        "The following cell contains the training data connection details.  \n",
        "**Note**: The connection might contain authorization credentials, so be careful when sharing the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ibm_watson_machine_learning.helpers import DataConnection\n",
        "from ibm_watson_machine_learning.helpers import FSLocation\n",
        "\ntraining_data_references = [\n",
        "    DataConnection(\n",
        "        data_asset_id='data_asset.COSCHURNcsv'\n",
        "    ),\n",
        "]\n",
        "training_result_reference = DataConnection(\n",
        "    location=FSLocation(\n",
        "        path='/projects/b93c5f96-2833-44e1-978a-50306a3b23f6/assets/auto_ml/auto_ml.AutoAIchurnmodel/wml_data/85813fc2-1c1d-49c9-b239-ba7e1dbd0c6d/data/automl'\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following cell contains input parameters provided to run the AutoAI experiment in Watson Studio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "experiment_metadata = dict(",
        "\n    prediction_type='binary'",
        ",\n    prediction_column='CHURN'",
        ",\n    holdout_size=0.48",
        ",\n    scoring='f1'",
        ",\n    csv_separator=','",
        ",\n    random_state=33",
        ",\n    max_number_of_estimators=4",
        ",\n    training_data_references=training_data_references",
        ",\n    training_result_reference=training_result_reference",
        ",\n    project_id='b93c5f96-2833-44e1-978a-50306a3b23f6'",
        ",\n    train_sample_columns_index_list=[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55]",
        ",\n    positive_label='1.00'",
        ",\n    drop_duplicates=True",
        ",\n    include_batched_ensemble_estimators=['BatchedTreeEnsembleClassifier(RandomForestClassifier)', 'BatchedTreeEnsembleClassifier(ExtraTreesClassifier)', 'BatchedTreeEnsembleClassifier(XGBClassifier)', 'BatchedTreeEnsembleClassifier(LGBMClassifier)', 'BatchedTreeEnsembleClassifier(SnapRandomForestClassifier)', 'BatchedTreeEnsembleClassifier(SnapBoostingMachineClassifier)']",
        ",\n    classes=[0.0, 1.0]",
        "\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set `n_jobs` parameter to the number of available CPUs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, ast\n",
        "CPU_NUMBER = 4\n",
        "if 'RUNTIME_HARDWARE_SPEC' in os.environ:\n",
        "    CPU_NUMBER = int(ast.literal_eval(os.environ['RUNTIME_HARDWARE_SPEC'])['num_cpu'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"connection\"></a>\n",
        "## Watson Machine Learning connection\n",
        "\n",
        "This cell defines the credentials required to work with the Watson Machine Learning service.\n",
        "\n",
        "**Action**: You need to pass your Cloud Pak for Data instance home url.   \n",
        "**Note**: If you want to run the notebook outside of the Watson Studio please provide Watson Machine Learning credentials: [Documentation](http://ibm-wml-api-pyclient.mybluemix.net/#id6)\n",
        "```\n",
        "wml_credentials = {  \n",
        "    \"instance_id\": \"openshift\",  \n",
        "    \"token\": \"\",  \n",
        "    \"url\": \"\",  \n",
        "    \"version\": \"4.6\"  \n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "wml_credentials = {\n",
        "    \"instance_id\": \"openshift\",\n",
        "    \"url\": os.environ.get(\"RUNTIME_ENV_APSX_URL\"),\n",
        "    \"version\": \"4.6\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ibm_watson_machine_learning import APIClient\n\nwml_client = APIClient(wml_credentials)\n\nif 'space_id' in experiment_metadata:\n    wml_client.set.default_space(experiment_metadata['space_id'])\nelse:\n    wml_client.set.default_project(experiment_metadata['project_id'])\n    \ntraining_data_references[0].set_client(wml_client)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"incremental_learning\"></a>\n",
        "# Incremental learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"preview_model_to_python_code\"></a>\n",
        "## Get pipeline\n",
        "\n",
        "Download and save a pipeline model object from the AutoAI training job (`lale` pipeline type is used for inspection and `partial_fit` capabilities)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ibm_watson_machine_learning.experiment import AutoAI\n",
        "\n",
        "pipeline_optimizer = AutoAI(wml_credentials, project_id=experiment_metadata['project_id']).runs.get_optimizer(metadata=experiment_metadata)\n",
        "pipeline_model = pipeline_optimizer.get_pipeline(pipeline_name='Pipeline_5', astype='lale')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"data_loader\"></a>\n",
        "## Data loader\n",
        "\n",
        "Create DataLoader iterator to retrieve training dataset in batches. DataLoader is `Torch` compatible (`torch.utils.data`), returning Pandas DataFrames.\n",
        "\n",
        "**Note**: If reading data results in an error, provide data as iterable reader (e.g. `read_csv()` method from Pandas with chunks). It may be necessary to use methods for initial data pre-processing like: e.g. `DataFrame.dropna()`, `DataFrame.drop_duplicates()`, `DataFrame.sample()`.\n",
        "\n",
        "```\n",
        "reader_full_data = pd.read_csv(DATA_PATH, chunksize=CHUNK_SIZE)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Batch size in rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "number_of_batch_rows = 29257"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ibm_watson_machine_learning.data_loaders import experiment as data_loaders\n",
        "from ibm_watson_machine_learning.data_loaders.datasets import experiment as datasets\n",
        "\n",
        "dataset = datasets.ExperimentIterableDataset(\n",
        "    connection=training_data_references[0],\n",
        "    enable_sampling=False,\n",
        "    experiment_metadata=experiment_metadata,\n",
        "    number_of_batch_rows=number_of_batch_rows\n",
        "    )\n",
        "    \n",
        "data_loader = data_loaders.ExperimentDataLoader(dataset=dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"train\"></a>\n",
        "## Continue model training\n",
        "\n",
        "In this cell, the pipeline is incrementally fitted using data batches (via `partial_fit` calls).\n",
        "\n",
        "**Note**: If you need, you can evaluate the pipeline using custom holdout data. Provide the `X_test`, `y_test` and call `scorer` on them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define scorer from the optimization metric\n",
        "This cell constructs the cell scorer based on the experiment metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import get_scorer, make_scorer\n\n",
        "scorer = get_scorer(experiment_metadata['scoring'])\n",
        "kwargs = {'pos_label':'1.00'}\n",
        "scorer = make_scorer(scorer._score_func, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tuning the incremental learner\n    \nFor the best training performance set:\n    \n- `n_jobs` - to available number of CPUs",
        "."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_model.steps[-1][1].impl.base_ensemble.set_params(n_jobs=CPU_NUMBER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set up a learning curve plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from ibm_watson_machine_learning.utils.autoai.incremental import plot_learning_curve\n",
        "import time\n",
        "\n",
        "partial_fit_scores = []\n",
        "fit_times = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"test_model\"></a>\n",
        "### Fit pipeline model in batches\n",
        "\n",
        "**Tip**: If the data passed to `partial_fit` is highly imbalanced (>1:10), please consider applying the `sample_weight` parameter:\n",
        "\n",
        "```\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "\n",
        "pipeline_model.partial_fit(X_train, y_train, freeze_trained_prefix=True,\n",
        "                                             sample_weight=compute_sample_weight('balanced', y_train))\n",
        "```                                                                                      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note**: If you have a holdout/test set please provide it for better pipeline evaluation and replace X_test and y_test in the following cell.\n                \n```\nfrom pandas import read_csv\ntest_df = read_csv('DATA_PATH')\n\nX_test = test_df.drop([experiment_metadata['prediction_column']], axis=1).values\ny_test = test_df[experiment_metadata['prediction_column']].values\n```\n\nIf holdout set was not provided, 30% of first training batch would be used as holdout.\n            \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 4))\n\nfor i, batch_df in enumerate(data_loader):\n    batch_df.dropna(subset=experiment_metadata[\"prediction_column\"], inplace=True)\n    X_train = batch_df.drop([experiment_metadata['prediction_column']], axis=1).values\n    y_train = batch_df[experiment_metadata['prediction_column']].values\n    if i==0:\n        X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.3)\n    start_time = time.time()\n    pipeline_model = pipeline_model.partial_fit(X_train, y_train, freeze_trained_prefix=True)\n    fit_times.append(time.time() - start_time)\n    partial_fit_scores.append(scorer(pipeline_model, X_test, y_test))\n    plot_learning_curve(fig=fig, axes=axes, scores=partial_fit_scores, fit_times=fit_times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"test_model\"></a>\n",
        "## Test pipeline model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test the fitted pipeline (`predict`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_model.predict(X_test[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"saving\"></a>\n",
        "## Store the model\n",
        "\n",
        "In this section you will learn how to store the incrementally trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_metadata = {\n",
        "    wml_client.repository.ModelMetaNames.NAME: 'Incrementally trained AutoAI pipeline'\n",
        "}\n",
        "\n",
        "stored_model_details = wml_client.repository.store_model(model=pipeline_model, meta_props=model_metadata, experiment_metadata=experiment_metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inspect the stored model details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stored_model_details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"deployment\"></a>\n",
        "## Create online deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can use commands bellow to promote the model to space and create online deployment (web service). \n",
        "\n",
        "<a id=\"working_spaces\"></a>\n",
        "### Working with spaces\n",
        "\n",
        "In this section you will specify a deployment space for organizing the assets for deploying and scoring the model. If you do not have an existing space, you can use [Deployment Spaces Dashboard](https://dataplatform.cloud.ibm.com/ml-runtime/spaces?context=cpdaas) to create a new space, following these steps:\n",
        "\n",
        "- Click **New Deployment Space**.\n",
        "- Create an empty space.\n",
        "- Select Cloud Object Storage.\n",
        "- Select Watson Machine Learning instance and press **Create**.\n",
        "- Copy `space_id` and paste it below.\n",
        "\n",
        "**Tip**: You can also use the API to prepare the space for your work. Learn more [here](https://github.com/IBM/watson-machine-learning-samples/blob/master/notebooks/python_sdk/instance-management/Space%20management.ipynb).\n",
        "\n",
        "**Action**: Assign or update space ID below."
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "space_id = \"PUT_YOUR_SPACE_ID_HERE\"\n",
        "\n",
        "model_id = wml_client.repository.promote_model(model_id=stored_model_details[\"metadata\"][\"id\"], source_project_id=experiment_metadata[\"project_id\"], target_space_id=space_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prepare online deployment"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "wml_client.set.default_space(space_id)\n",
        "\n",
        "deploy_meta = {\n",
        "        wml_client.deployments.ConfigurationMetaNames.NAME: \"Incrementally trained AutoAI pipeline\",\n",
        "        wml_client.deployments.ConfigurationMetaNames.ONLINE: {},\n",
        "    }\n",
        "\n",
        "deployment_details = wml_client.deployments.create(artifact_uid=model_id, meta_props=deploy_meta)\n",
        "deployment_id = wml_client.deployments.get_id(deployment_details)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test online deployment"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "scoring_payload = {\n",
        "    \"input_data\": [{\n",
        "        'values': pd.DataFrame(X_test[:5])\n",
        "    }]\n",
        "}\n",
        "\n",
        "wml_client.deployments.score(deployment_id, scoring_payload)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"cleanup\"></a>\n",
        "### Deleting deployment\n",
        "You can delete the existing deployment by calling the `wml_client.deployments.delete(deployment_id)` command.\n",
        "To list the existing web services, use `wml_client.deployments.list()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"summary_and_next_steps\"></a>\n",
        "# Summary and next steps\n",
        "You've successfully completed this notebook!\n",
        "You've learned how to use AutoAI pipeline definition to train the model.\n",
        "Check out the official [AutoAI site](https://www.ibm.com/cloud/watson-studio/autoai) for more samples, tutorials, documentation, how-tos, and blog posts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"copyrights\"></a>\n",
        "### Copyrights\n",
        "\n",
        "Licensed Materials - Copyright \u00a9 2022 IBM. This notebook and its source code are released under the terms of the ILAN License. Use, duplication disclosure restricted by GSA ADP Schedule Contract with IBM Corp.\n",
        "\n",
        "**Note:** The auto-generated notebooks are subject to the International License Agreement for Non-Warranted Programs (or equivalent) and License Information document for Watson Studio Auto-generated Notebook (License Terms), such agreements located in the link below. Specifically, the Source Components and Sample Materials clause included in the License Information document for Watson Studio Auto-generated Notebook applies to the auto-generated notebooks.  \n",
        "\n",
        "By downloading, copying, accessing, or otherwise using the materials, you agree to the <a href=\"http://www14.software.ibm.com/cgi-bin/weblap/lap.pl?li_formnum=L-AMCU-BYC7LF\">License Terms</a>\n",
        "\n",
        "___"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
